{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df91a0db-91e5-47e5-8505-9128efa8fc68",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 36px; color: green; font-weight: bold;\">Multi Head Attention</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ffc2af7-9bcf-42a6-a4f3-d80fe8e4c2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f49314cc-9c55-470f-aa71-60be84acef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention,self).__init__()    # Constructor of parent class, initializes nn.Module properly\n",
    "        assert d_model % num_heads == 0, \"dimension of model must be divisible by number of heads\"    #checks if dimension is compatible with number of heads\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)    #Linear Transformation applied to Queries, aka fully connected layer\n",
    "        self.W_k = nn.Linear(d_model, d_model)    #Linear Transformation applied to Keys, aka fully connected layer\n",
    "        self.W_v = nn.Linear(d_model, d_model)    #Linear Transformation applied to Values, aka fully connected layer\n",
    "        self.W_o = nn.Linear(d_model, d_model)    ##Linear Transformation applied to Output, aka fully connected layer\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(self.d_k)    #Computes attention score between each query and key, \"how much should this query attend to each key?\"\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)    #Applies mask, used to ignore padding tokens and prevent attention to future tokens. Replaces masked positions with a very negative number so that softmax is nearly 0\n",
    "        attn_probs = torch.softmax(attn_scores, dim = -1)    #Converts attention scores to probabilities that sum to 1. Tells how much attention to give each token for a query\n",
    "        output = torch.matmul(attn_probs,V)    #Applies attention weights to value vectors V. Gives a weighted sum of values, based on how relevant each key was to the query\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):    #X = (batch_size, seq_length, d_model). We want to split d_model into multiple heads to do attention in parallel.\n",
    "        batch_size, seq_length, d_model = x.size()    #Get dimensions\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1,2)    #Reshape to  (batch_size, num_heads, seq_length, d_k)\n",
    "\n",
    "    def combine_heads(self, x):    #Reverse of split_heads, this function takes the output from each attention head and merges them back into a single representation per token.\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):    #Performs full multi-head attention operation, which does: Input embedding projection, Splitting em into multiple heads, Computing attention per head, Combining output of each head and then passing it thru a final linear layer, to get one output vector.\n",
    "        Q = self.split_heads(self.W_q(Q))    #Takes the output of Linear layers inside and reshapes them into num_heads different heads. Output is batch_size, num_heads, seq_length, d_k) where d_k = d_model // num_heads.\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)    #Calculates attention independedntly for each head.\n",
    "        output = self.W_o(self.combine_heads(attn_output))    #Combines attention from all heads and converts to shape (batch_size, seq_length, d_model). Then applies final transformation W_o.\n",
    "        return output    #Output contains contextual representation of each token, given contextual info of other tokens its most related to, via attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2acb2a-f6ec-4173-bea0-e45cc89e2012",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 36px; color: green; font-weight: bold;\">Position-wise Feed-Forward Networks</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02ca0532-4c31-44d4-91e0-ec42fb3c3543",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):    #Initializes two linear transformation layers and a ReLU activation function, then applies em\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8d9541-d380-450f-b4e5-223d34fa1c80",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 36px; color: green; font-weight: bold;\">Positional Encoding</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d596247f-a237-4087-85d6-38aa1f8fbdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):    #Captures positional info of input sequence \n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)    #Create position indices (0, 1, 2, ..., max_seq_length-1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))    #Calculate scaling factor for sinusoidal frequencies, controls frequencey of sine/cosine curves\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)    #even dims, alternates for each dimension\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)    #odd dims, alternates for each dimension\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))    #register_buffer ensures it's not updated by gradients but still moves with .to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af319e0a-df8e-40cb-a283-af7837c5ffad",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 36px; color: green; font-weight: bold;\">Encode Layer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "760fb099-c828-4ddc-8dbe-7c2380bba24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:552/format:webp/0*bPKV4ekQr9ZjYkWJ.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://miro.medium.com/v2/resize:fit:552/format:webp/0*bPKV4ekQr9ZjYkWJ.png\"\n",
    "Image(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19fc8248-a6db-4a10-ac92-ac228e744835",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):    #The forward methods computes the encoder layer output by applying self-attention, adding the attention output to the input tensor, and normalizing the result. Then, it computes the position-wise feed-forward output, combines it with the normalized self-attention output, and normalizes the final result before returning the processed tensor.\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2363f58d-92c1-40bb-aa00-e45c5efd022a",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 36px; color: green; font-weight: bold;\">Decoder Layer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43dae570-8f31-4cb8-b094-f445dd8bdf85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:552/format:webp/0*SPZgT4k8GQi37H__.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://miro.medium.com/v2/resize:fit:552/format:webp/0*SPZgT4k8GQi37H__.png\"\n",
    "Image(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d6966f1-6172-43a4-b191-a4cf4aeeba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):    #Does operations as in diagram\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62874e1b-41da-4615-b07d-75a68ad4cfeb",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 36px; color: green; font-weight: bold;\">Transformer Model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f615882-e02c-4bdd-bc19-5854be0c8556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/0*ljYs7oOlKC71SzSr.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://miro.medium.com/v2/resize:fit:640/format:webp/0*ljYs7oOlKC71SzSr.png\"\n",
    "Image(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35d2c26c-d624-44f3-b723-a908e0627108",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)    #A mask for the source sequence, used to ignore padding tokens in the source sequence (tokens with value 0)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)    \n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask    ##A mask for the target sequence, used to ignore padding tokens in the target sequence. Additionally, it prevents attending to future tokens by creating a \"nopeak\" mask.\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):    #src is source sequence, same for tgt\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))    #Dropout used to prevent overfitting via regularization \n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:    #Iterates over multiple layers of encoders\n",
    "            enc_output = enc_layer(enc_output, src_mask)    #Contains the final representation of the source sequence\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)    #Contains the final representation of the target sequence after passing through all decoder layers.\n",
    "\n",
    "        output = self.fc(dec_output)     #produces logits for each token in the target sequence, where each value represents the model's confidence in predicting a particular token from the target vocabulary.\n",
    "        return output    #shape (batch_size, seq_length, tgt_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1012ae-6fee-4b64-af22-ae37cedfec62",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 36px; color: green; font-weight: bold;\">Sample Data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c413d7d-775c-4f97-bca6-ed25bdd13dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b278f63-9370-4890-bd45-58dc7b8c9b1f",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 36px; color: green; font-weight: bold;\">Training Model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e6a6d65-0e4e-4287-a3ff-dfbd53eb1f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 7.436934471130371\n",
      "Epoch: 2, Loss: 7.669724464416504\n",
      "Epoch: 3, Loss: 7.408854007720947\n",
      "Epoch: 4, Loss: 7.322382926940918\n",
      "Epoch: 5, Loss: 7.24190092086792\n",
      "Epoch: 6, Loss: 7.166414737701416\n",
      "Epoch: 7, Loss: 7.100143909454346\n",
      "Epoch: 8, Loss: 7.026224136352539\n",
      "Epoch: 9, Loss: 6.9529852867126465\n",
      "Epoch: 10, Loss: 6.8840179443359375\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e34b87-2e80-484c-87b5-abc5973a6845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#courtesy : https://medium.com/data-science/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6ImM3ZTA0NDY1NjQ5ZmZhNjA2NTU3NjUwYzdlNjVmMGE4N2FlMDBmZTgiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMDAzMDcyMjA5NzM1OTYyMDE1NjAiLCJlbWFpbCI6InJpdHZpa3NoYXJtYTk5QGdtYWlsLmNvbSIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJuYmYiOjE3NDQ0NTU2NzYsIm5hbWUiOiJSaXR2aWsgU2hhcm1hIiwicGljdHVyZSI6Imh0dHBzOi8vbGgzLmdvb2dsZXVzZXJjb250ZW50LmNvbS9hL0FDZzhvY0lxNFluQWtnb1dWYzB3TkVtRDhZQ2NQVjgzSGhvYUh3aDlpSlR1NTNKa2JXVDZ2dzA9czk2LWMiLCJnaXZlbl9uYW1lIjoiUml0dmlrIiwiZmFtaWx5X25hbWUiOiJTaGFybWEiLCJpYXQiOjE3NDQ0NTU5NzYsImV4cCI6MTc0NDQ1OTU3NiwianRpIjoiODliOGRkNjEwODNhZDA5MmQ0MzhlNmY2OWVhMmRkMjMyODRmMjYyMCJ9.6U8H9wRErFtqfOCyBz_3RmqXTmQ_VqxXbj5E9ozwQsltwiyv8NIPvhVA5CEUSkhHkuLA3xO7uITpdtRlB3PV_2OhVdG1umTNAXom_5BwaeDK-tTZuhVwNYkJPxWMOjYkpMEzgHSOUV0XnVXJAUjLhCYfH7J9rGnyXmFxsjPE5_4kP5mp7bU6E8V9i83T1dTNbVys0CyaTtfhXVwPG1WNXZjS9X1Y-eqeAUhmn0sXfB-fw7-0sH8IrRcbaG8thvqrKC8PcumxRAGCC8UbU0dN064muaGDSgermMDtbsjNeBOz4rkpoeYxI2h4pesEs74fOJVzyZWUefbu60AKllOzpQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f47bc-5281-4602-b045-e1e8a03c04a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
